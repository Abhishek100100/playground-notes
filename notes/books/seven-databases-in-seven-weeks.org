* Seven Databases in Seven Weeks

** PostgreSQL

- `CROSSTAB` - Pivoting the table.
- `RULE` - Description how to alter the parsed query tree.
  - In practice it opens various possibilities like manipulating the
    `VIEW` directly by adding or updating columns and proceeding
    with proper action after change.
- `TRIGGER` - Function or procedure invoked after table modification.
- `OVER` - Window Query which groups but have less strict requirements
  than `GROUP BY`.
- `EXPLAIN` - Execution plan and estimated query cost with
  detailed description.

*** Additional modules

- `postgresql::cube` - For representing multidimensional cubes.
- `metaphones`, `trigraphs`, `soundex`, Levenshtein distance.

** Riak

Distributed, highly available key-value storage inspired by Amazon Dynamo DB.

- Accessible via HTTP and REST interface like:
  - `curl http://localhost:8091/ping`
  - `curl http://localhost:8091/stats`
But it has drivers to access it without involving HTTP protocol.

The whole concept of scaling is based on hash ring and splitting cluster
(set of nodes) into almost equal partitions (by default 64 virtual nodes,
so for three partitions two nodes have got 21 virtual nodes, one 22 virtual
nodes).

Adding more nodes to the cluster will involve repartitioning and spreading
data into the rest of the cluster. With commands and queries there are three
connected attributes - `R/W/N`:
- `R` stands for reads, from how many nodes we should have read the
  same value before returning to be sure that read is successful and consistent.
- `W` stands for writes, how many of them should be performed before
  we return to the user with success.
- `N` stands for nodes in the cluster.

*** Riak in intelligent way bends the CAP theorem

Consistency, Availability, Partition Tolerance - there is no distributed
system which have strong guarantees for all of them at the same time, you
have to pick two and you have to always consider partition tolerance.

The real choice is between consistency and availability.

In the Riak approach the crucial part is "at the same time" with mentioned
three values we can exchange `C` over `A` on per-request, per-replication,
per-* basis. In other words we are reducing to the minimum time when system
is switching between states and from the outside it looks like a system which
have "strong" guarantees in terms of all of three requirements from this
theorem.

There is no single point of failure in Riak. Even vector clocks and merging
conflicts features are created with full awareness of scaling.

Riak have secondary indexes over not values, but metadata.

Riak is extensible, you can e.g. enable `riak_index` which is a `Solr` back-end
for searching inside database.

Riak supports two languages for map-reduce algorithms and pre/post-commit hooks:
- Erlang
- JavaScript

** HBase

Apache HBase is an open-source, distributed, versioned, non-relational database
modeled after Google's Bigtable Engine. It is a database back-end for Hadoop. It
is designed to handle many gigabytes of storage.

Use Apache HBase when you need random, real-time read/write access to your
Big Data. This project's goal is the hosting of very large tables - billions of
rows times millions of columns - atop clusters of commodity hardware.

There are multiple modes:
- Standalone mode.
- Faked cluster (multiple instances simulated on one machine).
- Purely distributed cluster (multiple instances on multiple machines).

HBase by default uses WAL (write-ahead log, journaling) as a recovery strategy.
There are two compression algorithms available in the HBase: GZ and LZO.
However LZO license is not compatible with Apache License so you need to
install it manually.

HBase uses Bloom Filters to produce faster lookups. There are constant space
size structures, opposing to the hashes. However they can introduce false
positives when too many elements is inserted to that bucket (too huge
generalization of filter).

HBase spreads the wings at scale. It is a minimum number of 5 clusters that
community chosen to use when you deploy it in the cluster.

There are various tools which helps managing such farms - Apache Whirr
(for creating, destroying, administrating and maintaining clusters in
the cloud - on EC2 or Rackspace), Apache Zookeeper (internode
communication mechanism and supervisor) etc.

HBase should be used when your data storage will be measured in
dozens of GB and TB.

** MongoDB

Name came from word `humongous` and applies to amount of data stored
in this NoSQL database. Mongo underneath uses binary JSON (BSON) format
for storing data.

JavaScript is a language of choice for data manipulations, queries
and map-reduce batches.

There are no server-side joins support in Mongo - it is a plain document
database, schemaless and what is more important - it suits huge and small
amount of data with easily ad-hoc querying mechanism.

There are collections (like buckets in Riak and tables in SQL) and
each item in the collection is called a document.

ObjectID is an internal way of handling primary keys compressed
into 12 bytes. It consists of:
- Timestamp (4 bytes)
- Machine ID (MID, 3 bytes).
- Process ID (PID, 2 bytes).
- Counter (3 bytes).

*** Querying - Facts and Tricks

- Regarding querying it has full support for PCRE.
  - Perl-Compatible Regular Expressions.
- It can dig deeper into document by dot operator
  - Like 'mayor.party'.
- For exact comparison Mongo has $elemMatch operator.
- For existence testing it has $exists operator.
- You can use even your own function predicated with
  exact code and logic inside to query.
  - It is slow!
  - It can't be optimized!

*** Indexes

- MongoDB uses BTree indexes and many more other techniques to
  reduce searching time and increase the querying speed.
- MongoDB has also complicated engine for explaining query execution
  plan and profiling mechanisms for investigation what is going on.
- Indexes are accessible inside collection or in the database system collection.

*** Replicasets

- Set of nodes which act like a one.
- Secondary and Primary replicas.

*** Sharding

- Three different responsibilities:
  - Configuration server which holds information about distribution.
  - Router which redirects requests to the proper shard
  - Slaves - which represents the shards.

*** GridFS
- Distributed file system based on MongoDB and actual collections.
  - Files are stored in separate collection, metadata in another one.

** CouchDB

Distributed document database with automatic conflicts resolution and with use
JSON storage for documents, JavaScript for map-reduce indexes, and regular HTTP(S)
for its REST API.

CouchDB has it own web interface called Futon. You can easily administrate users,
perform CRUD operations etc. The same operations can be done from any kind of
client which supports HTTP. All drivers are using REST API, there is no additional
protocol to support.

CouchDB can easily scale up and down without additional overhead and also run
over various platforms (like mobile phones, clusters, servers, embedded,
desktop) e.g. there is a project called `PouchDB` for creating `CouchDB`-like
capabilities in the browser, in client-side JavaScript.

*** Views

Temporary views are very slow and should be used only for internal development.
Proper views are internally a database object which can have customized map and
reduce steps implemented in JavaScript. You can create views via Futon and via
REST API.

What is more important - you can use multiple layers of steps (e.g. map, map,
reduce, reduce) and each should be written with use of JavaScript.

After defining a view and creating initial representation of it, when we insert
new or modify old value - view will be recalculated on demand. It is a primary
indexing mechanism, which can store also intermediate values.

What makes CouchDB unique is a Changes API. It Is a mechanism of watching database
changes and getting updates instantly. You can poll them and filter the not
interesting values.

CouchDB supports replication and offline mode.

** Neo4j

- Not fully open sourced graph database.
  - Alternative - https://www.arangodb.org.
- Community edition has most of enterprise features, however clustering and
  high availability Is available only in the enterprise version.
- Database have pretty impressive web administration panel with visualizations.
- Database domain language isn't very popular and actually is different
  than other databases.
- There are several ways of querying Neo4j:
  - Gremlin - DSL based on Groovy for traversing graphs.
    - `g` is a graph object representation, which has:
      - `E` as an edges set.
      - `V` as a vertices set.
    - It supports piping requests.
    - You can define your own steps definition.
  - Java driver.
  - Ruby driver.
  - Cypher - Different DSL for querying based on pattern matching.
    - It is similar to the SQL.
  - REST API.
- Database supports various of algorithms, metrics and graphs representation.
- Indexing is supported by external service, however indexes works as
  in the other databases.
- Gremlin supports transactions, by Blueprint extension. However Neo4j works
  with HA and PT, with losing a C from CAP theorem. So even by having
  transactions it doesn't mean that after committing transaction we will
  have consistent state shared across master and slaves.

** Redis

Redis is often used like a lubricant in the complex machine. It has lower operational
and infrastructural overhead and it is more powerful than memcached.

It is effectively more than key-value store, it supports TTL, complex data types
without nesting and transactional model. Moreover, it can replicate with master-slave
manner.

Data Structures and Types:
- Lists (also with capped capacity).
- Hashes.
- Sets and sorted sets.
- Strings.
- Integers.

Redis provides simple authentication model and ability to rename certain commands.
It is possible to even disable completely certain commands rebinding them
to empty string. Nevertheless authors strongly recommends securing it on
infrastructural level at first.

Redis provides several persistence mechanisms suitable for different types of
caching servers, even very picky in terms of durability (appendonly logging
mechanism similar to write-ahead logging).

It is very easy to setup master-slave replication mechnism.

More complicated stuff is doable but requires additional implementation
on client side level (e.g. consistent ring with hashing).

The same level of difficulty is to setup a publish-subscribe mechanism
based on channels in Redis.
