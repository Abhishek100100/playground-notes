* Node.js Patterns, Pedro Teixeira

** Module Patterns

- CommonJS standard.
- Difference between `exports.foo` and `module.exports`.
  - In first case we are extending the `module.exports`
    object. `module` is an implicit part, however in second case if we
    would like to override whole object, we cannot avoid that because
    of JavaScript name resolution.
- Requiring a module - always remember about './' in the path.
  - In some cases lack of it will produce an error about `ENOENT`.
- Module types:
  - One function (`module.exports = foo;`).
  - Singleton object.
    - Very specific example of this is concentrated module, which
      connects multiple modules into one.
  - Closure-based class (starting with capital letter).
  - Prototype-based class.
  - Facade module.
    - Grouping similar responsibilities under the single structure.
- *Browserify*.
  - Unified dependency mechanism, with bundling for Node.js and
    Browser.

** Flow Control Patterns

- Asynchronous I/O built with callbacks is hard to compose.
- Node.js uses callbacks to deal with I/O continuations on lowest
  level:
  - One callback (instead of *success* and *error* handler).
  - Once called (*callback* should be called only once).
  - Function-last (*callback* is a last argument of I/O operation).
  - Error-first (*error* is a first argument of callback).
    - If error occurs, *results* are *undefined*.
    - If no error happened, *error* is *null* or *undefined*.
- Orchestration.
  - `npm install async`.
  - Examples:
    - `async.series` - invoke multiple asynchronous calls one after
      another. Error in one function from a sequence breaks the flow,
      and invokes the final callback.
    - `async.parallel` - invoke multiple asynchronous calls
      concurrently. The first erroneous call will invoke the callback.
    - `async.waterfall` - sequence of asynchronous operations that
      transforms the result from first one, and passes it to the
      second and so on. Error handling like in `async.series`.
    - Collections
      - Parallel iteration (`async.each`).
      - Parallel iteration with limits (`async.eachLimit`).
      - Why use a limit?
        - Avoid overloading the recipient.
        - Avoid overloading heap allocations.
        - Avoid blocking event loop for a small amount of time
          (e.g. 10 ms for putting element to the event loop queue
          times 1000 elements in collection and of course -
          iterating is blocking as well).
    - Work Queues.
      - Reducing overload of external system.
      - To perform asynchronous work.
      - `async.queue`.
- Besides callbacks Node.js has also the events - `EventEmitter`.
  - It wraps the state change notification patterns.
  - Decouples consumers and producers (*internal pub/sub*).
  - `require('events').EventEmitter` and always use `util.inherits`.
  - If an error happens, and no one is subscribed on the `error`
    event, it will be returned to the event loop, which causes
    `uncaughtException` generation.
  - Since `EventEmitter` is an internal events mechanism and it
    involves no I/O operations events delivery is synchronous.
    - So subscribe on certain events before emitting the first value.
    - What is even more important, if one of the listeners throws rest
      of subscribed listeners may not receive the event (it depends on
      the internal order). That is a consequence of being synchronous
      in events delivery mechanism.
- Streams as a platform primitive.
  - Source and targets of data.
  - `require('stream');`
  - `objectMode` - mode which handles data which are not a strings or
    buffers. By default disabled in the prototypes.
  - Types of streams:
    - Readable - `EventEmitter` is a special kind of *readable stream*.
      - `stream.Readable` prototype.
      - Pull stream - like e.g. drinking straw.
        - You can achieve it by calling `read()` at your own pace. It
          is necessary to justify properly the `highWaterMark` option
          at the beginning.
      - Push stream - like e.g. water tap.
        - Standard way of handling streams in that way is to *listen*
          on data event.
    - Writable
      - `stream.Writable` prototype.
      - You can push data into it via `stream.write(chunk);'.
    - Transform
      - `stream.Transform` prototype.
      - Stream which transforms all chunks that are flowing through.
  - Streams can be composed with `pipe()`.
    - `readable.pipe(writable);`
    - Flow control for free.
  - Streams polymorphism.
    - Feature detection (checking in the stream method implementation
      if certain properties are available and deciding on an action
      inside stream implementation).
    - Pipe overloading (each prototype can override the pipe method,
      and then do their stuff and forward call to the
      `Stream.prototype.pipe`).
    - Both mechanisms can be found in *mikeal/request* module.

** Configuration Patterns

- Singleton configuration module.
  - Directory or single file with values exported as an object.
- Environment variables.
- Cascading configuration.
  - Module: *konphyg*.
  - It can be used in each of three flavors, and progressively
    enhanced when needed.
- Using *CouchDB* to store and propagate configuration changes.
  - *CouchDB* is a key-value store where values are arbitrary JSON
    documents. It uses documents versions, it can be scaled and
    replicated easily.
  - It exposed a *REST* *HTTP* interface, with or without
    authentication so integration with it is pretty straightforward.
  - By using *follow* module you can achieve easy node synchronization
    after main repository updates. *Follow* will track changes on the
    *CouchDB* side and updates its internal singleton configuration
    module. Also it can emit events, that everyone interested in
    changes can subscribe as well.
  - Drawbacks:
    - Single point of failure, you need to manage your *CouchDB*
      instance.
- Propagating configuration changes using Conflict-free Replicated
  Data Types (*CRDTs*).
  - Has all benefits of previous solution, but eliminates *single
    point of failure*. It is based on proper data structures (*CRDT*)
    and gossiping between nodes about configuration state.
  - Modules: *dominictarr/crdt*, *reconnect-net*.
    - Secured from *reconnect storm* after node or network failure.
  - Topology: plain ring, full mesh (this introduces additional
    overhead in clusters with many nodes).

** Networking Patterns

- Usage of web frameworks have some benefits and drawbacks.
  - It is easier to create and start an application.
  - But it is also easier to build a *monolithic application*.
  - With Node.js you can easily build distributed applications,
    separate your concerns and isolate services (which has certain
    benefits related with structure and scaling), but also you can use
    web frameworks to get up to speed.
- Networking Patterns:
  - Plain sockets.
    - Just after each `error` event it will appear a `close` event.
    - All implementations based on handling events does not support
      any kind of back-pressure *out of the box*. Instead implementing
      it manually in most cases it is easier to write a custom
      *Stream* prototype implementation.
  - Streaming services.
    - How to stream JSON? Use *json-dulpex-stream* module and object
      mode for streams. Each *new line* will be interpreted as a new
      JSON object which can be properly decoded as a whole.
  - Remote Emitter pattern.
    - Module: *duplex-emitter*.
    - Using `EventEmitter` but remotely, over the network.
    - Handling re-connections - again *reconnect-net*.
      - It implements *exponential back-off* algorithm for
        reconnecting.
