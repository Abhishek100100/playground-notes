* Node.js Patterns, Pedro Teixeira

** Module Patterns

- CommonJS standard.
- Difference between `exports.foo` and `module.exports`.
  - In first case we are extending the `module.exports`
    object. `module` is an implicit part, however in second case if we
    would like to override whole object, we cannot avoid that because
    of JavaScript name resolution.
- Requiring a module - always remember about './' in the path.
  - In some cases lack of it will produce an error about `ENOENT`.
- Module types:
  - One function (`module.exports = foo;`).
  - Singleton object.
    - Very specific example of this is concentrated module, which
      connects multiple modules into one.
  - Closure-based class (starting with capital letter).
  - Prototype-based class.
  - Facade module.
    - Grouping similar responsibilities under the single structure.
- *Browserify*.
  - Unified dependency mechanism, with bundling for Node.js and
    Browser.

** Flow Control Patterns

- Asynchronous I/O built with callbacks is hard to compose.
- Node.js uses callbacks to deal with I/O continuations on lowest
  level:
  - One callback (instead of *success* and *error* handler).
  - Once called (*callback* should be called only once).
  - Function-last (*callback* is a last argument of I/O operation).
  - Error-first (*error* is a first argument of callback).
    - If error occurs, *results* are *undefined*.
    - If no error happened, *error* is *null* or *undefined*.
- Orchestration.
  - `npm install async`.
  - Examples:
    - `async.series` - invoke multiple asynchronous calls one after
      another. Error in one function from a sequence breaks the flow,
      and invokes the final callback.
    - `async.parallel` - invoke multiple asynchronous calls
      concurrently. The first erroneous call will invoke the callback.
    - `async.waterfall` - sequence of asynchronous operations that
      transforms the result from first one, and passes it to the
      second and so on. Error handling like in `async.series`.
    - Collections
      - Parallel iteration (`async.each`).
      - Parallel iteration with limits (`async.eachLimit`).
      - Why use a limit?
        - Avoid overloading the recipient.
        - Avoid overloading heap allocations.
        - Avoid blocking event loop for a small amount of time
          (e.g. 10 ms for putting element to the event loop queue
          times 1000 elements in collection and of course -
          iterating is blocking as well).
    - Work Queues.
      - Reducing overload of external system.
      - To perform asynchronous work.
      - `async.queue`.
- Besides callbacks Node.js has also the events - `EventEmitter`.
  - It wraps the state change notification patterns.
  - Decouples consumers and producers (*internal pub/sub*).
  - `require('events').EventEmitter` and always use `util.inherits`.
  - If an error happens, and no one is subscribed on the `error`
    event, it will be returned to the event loop, which causes
    `uncaughtException` generation.
  - Since `EventEmitter` is an internal events mechanism and it
    involves no I/O operations events delivery is synchronous.
    - So subscribe on certain events before emitting the first value.
    - What is even more important, if one of the listeners throws rest
      of subscribed listeners may not receive the event (it depends on
      the internal order). That is a consequence of being synchronous
      in events delivery mechanism.
- Streams as a platform primitive.
  - Source and targets of data.
  - `require('stream');`
  - `objectMode` - mode which handles data which are not a strings or
    buffers. By default disabled in the prototypes.
  - Types of streams:
    - Readable - `EventEmitter` is a special kind of *readable stream*.
      - `stream.Readable` prototype.
      - Pull stream - like e.g. drinking straw.
        - You can achieve it by calling `read()` at your own pace. It
          is necessary to justify properly the `highWaterMark` option
          at the beginning.
      - Push stream - like e.g. water tap.
        - Standard way of handling streams in that way is to *listen*
          on data event.
    - Writable
      - `stream.Writable` prototype.
      - You can push data into it via `stream.write(chunk);'.
    - Transform
      - `stream.Transform` prototype.
      - Stream which transforms all chunks that are flowing through.
  - Streams can be composed with `pipe()`.
    - `readable.pipe(writable);`
    - Flow control for free.
  - Streams polymorphism.
    - Feature detection (checking in the stream method implementation
      if certain properties are available and deciding on an action
      inside stream implementation).
    - Pipe overloading (each prototype can override the pipe method,
      and then do their stuff and forward call to the
      `Stream.prototype.pipe`).
    - Both mechanisms can be found in *mikeal/request* module.

** Configuration Patterns

- Singleton configuration module.
  - Directory or single file with values exported as an object.
- Environment variables.
- Cascading configuration.
  - Module: *konphyg*.
  - It can be used in each of three flavors, and progressively
    enhanced when needed.
- Using *CouchDB* to store and propagate configuration changes.
  - *CouchDB* is a key-value store where values are arbitrary JSON
    documents. It uses documents versions, it can be scaled and
    replicated easily.
  - It exposed a *REST* *HTTP* interface, with or without
    authentication so integration with it is pretty straightforward.
  - By using *follow* module you can achieve easy node synchronization
    after main repository updates. *Follow* will track changes on the
    *CouchDB* side and updates its internal singleton configuration
    module. Also it can emit events, that everyone interested in
    changes can subscribe as well.
  - Drawbacks:
    - Single point of failure, you need to manage your *CouchDB*
      instance.
- Propagating configuration changes using Conflict-free Replicated
  Data Types (*CRDTs*).
  - Has all benefits of previous solution, but eliminates *single
    point of failure*. It is based on proper data structures (*CRDT*)
    and gossiping between nodes about configuration state.
  - Modules: *dominictarr/crdt*, *reconnect-net*.
    - Secured from *reconnect storm* after node or network failure.
  - Topology: plain ring, full mesh (this introduces additional
    overhead in clusters with many nodes).

** Networking Patterns

- Usage of web frameworks have some benefits and drawbacks.
  - It is easier to create and start an application.
  - But it is also easier to build a *monolithic application*.
  - With Node.js you can easily build distributed applications,
    separate your concerns and isolate services (which has certain
    benefits related with structure and scaling), but also you can use
    web frameworks to get up to speed.
- Networking Patterns:
  - Plain sockets.
    - Just after each `error` event it will appear a `close` event.
    - All implementations based on handling events does not support
      any kind of back-pressure *out of the box*. Instead implementing
      it manually in most cases it is easier to write a custom
      *Stream* prototype implementation.
  - Streaming services.
    - How to stream JSON? Use *json-dulpex-stream* module and object
      mode for streams. Each *new line* will be interpreted as a new
      JSON object which can be properly decoded as a whole.
  - Remote Emitter pattern.
    - Module: *duplex-emitter*.
    - Using `EventEmitter` but remotely, over the network.
    - Handling re-connections - again *reconnect-net*.
      - It implements *exponential back-off* algorithm for
        reconnecting.
  - Multiplexing streams.
    - Module: *mux-demux*.
    - on the server-side, multiplex multiple streams into one.
    - on the client-side, demultiplex one stream into many.
    - It allows sending multiple types of events via one connection.
  - RPC (*Remote Procedure Call*).
    - Module: *dnode*.
    - If you already have streaming in your server, you can multiplex
      as well one of the channels related with *RPC*.
  - Encrypting the communication.
    - Creating *CA* (Certification Authority) with which we will
      create and sign further client and server certificates:
      - Private key:
        - `openssl genrsa -out private-key.pem 2048`
      - Public certificate signed with previous private key:
        - `openssl req -x509 -new -nodes -key private-key.pem -days 1024
                       -out certificate.pem
                       -subj "/C=PL/ST=Silesia/L=Gliwice/O=Me/CN=abcd.com"`
    - Server side certificate:
      - New private key: `openssl genrsa -out private-key.pem 2048`
      - *CSR* (Certificate Signing Request):
        - `openssl req -new -key private-key.pem -out csr.pem`
      - Self-signing the new server certificate with previously
        created CA (valid 500 next days):
        - `openssl x509 -req -in csr.pem
                        -CA ../certificate.pem -CAkey ../private-key.pem
                        -CAcreateserial -out certificate.pem -days 500`
    - Node.js `tls.createServer(options)` will emit two events
      `connect` and `secureConnection`. The first one *DOES NOT*
      provide an encrypted socket.
      - Option `rejectUnauthorized` set to `true` verifies the server
        authority from the client's perspective. In order to do that
        you need to supply the *CA* public certificate.
    - You can easily authenticate the client from the server points of
      view by setting up new certificate:
      - New private key for client:
        - `openssl genrsa -out private-key.pem 2048`
      - New *CSR* for client:
        - `openssl req -new -key private-key.pem -out csr.pem`
      - Public certificate signed with previous private key:
        - `openssl x509 -req -in csr.pem -CA ../root/certificate.pem
                        -CAkey ../root/private-key.pem
                        -CAcreateserial -out certificate.pem -days 500`
      - Then you need to add `key` and `cert` to your client and
        enforce verification of client's certificate on server side
        by:
        - Option `requestCert` set to `true`.
        - Option `rejectUnauthorized` set to `true` (which verifies
          the authenticity of the client's certificate).
      - Both server and client should share the same *CA*.
      - Once signed, certificate is valid forever - in order
        invalidate that you need to keep a white-list around the
        server and verify *CN* after each new connection to the
        server:
        - `connection.getPeerCertificate().subject.CN`
        - Instead, you can use *CRL* (Certificates Revocation List) and
          provide it to the server.
        - Remember that both solutions requires a server's restart,
          you can implement *gossiping* protocol which updates all
          nodes when something change automatically, as stated in
          [[Configuration Patterns]].

** Work Queue Patterns

- In most use cases related with web applications, queues are
  off-loading main *HTTP* request-response cycle. But this is not the
  only case.
  - The main case in all approaches is to decouple work *producer*
    from *consumers*. You can distribute your actions, load through
    time and space (nodes).
- Main features of work queues:
  - Isolation (isolate qfaults and 3rd party service providers from
    the application logic).
  - Retry (idempotent actions can be retried without any problem, also
    you are decoupling the main logic from your retries).
    - Even non-idempotent actions can benefit from being off-loaded to
      the work queue - by identifying them uniquely you can decide
      about retrying such operation or not.
  - Scaling (decoupling increases the ability to scale components
    individually, also such infrastructure can handle load peaks).
    - Node.js is not good in CPU-bound type operations, so your
      consumers can be written in different language, on different
      platform more suitable to do such job.
    - Remember that if any job looks as I/O bound, after introducing
      something more suitable for such job, it can dramatically evolve
      in terms of XYZ-bound type (e.g. initially parsing huge JSON
      payload was a I/O-bound operation, but after handing that to
      Node.js it turns out that I/O is no more a problem, but parsing,
      which is a CPU-bound operation).
  - Survive crashes, deployments, reboots.
- Use Cases:
  - *In-memory Work Queue*.
    - Modules: *async* (`async.queue`) and *backoff*.
    - Introducing work concurrency at level 1 and implementing
      back-off algorithm for reconnection you can easily implement
      your own in-memory queue implementation.
    - Remember about `queue.drain` event when disconnecting.
  - *Persisted Local Work Queue*.
    - Local instance of LevelDB will be a storage for incoming and
      pending events (both for audit and persisting the work to be
      done).
    - Module: *level-jobs* (it has *back-off* algorithm inside, you
      are passing only the level client instance in order to specify
      how and where to save the data).
  - *Distributed Work Queue*.
    - Instead of storing and providing scalability and fault isolation
      on the process level (at one machine) we can introduce
      networking and multiple machines, which increases scalability
      and fault-tolerance of whole solution.
      - But it will bring new problems to the table as well!
    - Queue implementation on top of *Redis*.
      - Module: *simple-redis-safe-work-queue*.
        - `Queue.worker` is a consumer.
        - `Queue.client` is a producer.
        - Retries are configurable by `workerOptions.maxRetries`. If
          you want to deliver something at most once, set this to 0.
        - Worker emits an event 'max retries' when you hit the limit.
        - Worker will try to do a callback until it succeeds or hit
          the retry limit.
    - Using *RabbitMQ* and *AMQP*.
      - Modules: *amqplib*.
      - Standard approach, nothing new (*ACK* a message when
        consumption ready and succeeded).

** Database Patterns

*** LevelDB

- *LevelDB* - small, key-value, embedded database.
  - `put`, `get`, `delete`, atomic batch operations.
  - Multiple encoding types for keys and values.
  - Streaming and using ranges or limits.
  - Partitioning keys by hand (and using `\xFF` when searching via
    range) or using module *level-sublevel* to do it on the logical
    level.
    - *level-sublevel* lets you update with batch operations
      atomically on many levels.
    - This modules has also hooks for e.g. invoked before each
      operation on specific sub-level will start.

*** Redis

- *Redis* - Key-value, in-memory and persistent database.
  - It has more than strings stored as a value e.g. sets, lists, TTL
    for keys etc.
  - `redis.multi()` is an approach to do transactions. *multi* starts
    a transaction, *exec* ends it.
  - *multi* does not provide atomic operations, because classic
    *get-set* is still vulnerable to the multiple clients updates.
    - In order to do that, you have to do *multi* with *watch*.
    - You are starting to observe a key by *watch*, invokes *get*, in
      the callback you should start *multi* with a *set*. If any
      conflict will be detected you can react properly. Obviously you
      should clean-up after by doing *unwatch*.
    - Another way to provide transactions is to use *Lua scripts* and
      invoke them in the database.
      - `redis.eval()` for the rescue. If you would like to cache, use
        `redis.evalsha()` and send this script hashed with use of
        *SHA1*. But remember - in order to use hash, you need to first
        evaluate script once (it should be in database memory).
  - Use Cases:
    - Using counters with *TTL* in order to implement API usage
      limits. You can use many atomic operations on integers inside
      the commands list e.g. *INCR*, *DECR*, *INCRBY* and so on.
    - You can store under single key a *hash* (dictionary).
    - You can store under single key a *list*.
      - On top of it you can implement of course lists, but also
        stacks and queues (there are commands for doing push and pops
        from both sides).
      - It has even more complicated commands like *BRPOPLPUSH* which
        pops and element from one list and atomically pushes it to
        another.
    - Redis also has *sets* - storage for unsorted, multiple
      values. By definition it will not hold the repeated values.
      - This data type has quick membership and intersection tests.
      - What if you would like to have sorted sets? Use *Z-prefixed*
        commands on sets, which creates and uses standard sets, but
        each element has also a *score* (natural integer) which is a
        sorting key.
  - Besides data types, Redis has also *publish-subscribe* mechanism,
    which can be use in e.g. inter-process communication
    - You can implement on top of that *distributed EventEmitter* for
      Node.js.
    - It is not a persistent queue! Each subscriber will receive only
      messages that were send after particular subscription happened.

*** CouchDB

- *CouchDB* - Key-value, distributed storage with conflict resolution.
  - Each value is a JSON document, each has revision and unique
    identifier. It has fully RESTful API to communicate with it.
    - You can enforce your own identifier, if not - it will chosen for
      you by database.
  - Module: *nano* (it provides really small, but for most cases
    complete wrapper for REST API).
    - Remember to tune up the `http.globalAgent.maxSockets` value.
  - Structure - documents are stored with a key, inside a database.
    - If we try to store a document with the same ID, but passing
      different revision number, we will receive `409` error and
      client need to resolve conflict on its side.
  - Even if the there is no enforced schema, you can prepare one and
    you can enforce it where you want, but on the client-side.
    - Module: *joi*.
    - Now you have got problems with different error
      representations. If you consider to server your API over HTTP
      anyway, please look at this module, which unifies the errors (by
      wrapping them, not replacing) under the common representation
      close to the HTTP.
      - Module: *boom*.
  - Doing conflict resolution on the client-side can be tricky. There
    are many different scenarios (full override, partial override,
    restricting some fields, merging two documents).
    - Module: *object-versions*.
  - In CouchDB if you want to query something different than ID, you
    need to create an *indexed view* (it won't let you perform a slow
    query).
    - Views are *map-reduce* transformations written in JavaScript.
    - Each view is represented with something called *design doc*.
    - Before using a view, you need to create it by putting the
      prepared design document (JavaScript file) into proper REST API
      endpoint.
    - Beware: views are *materialized views* it means that they are
      created upfront. On big data sets each view change can freeze
      your database, it may become unresponsive.
    - Paginating views - do not specify *limit* and *offset* (CouchDB
      stores internally records as *B-Trees* so by specifying
      pagination like this it needs to scan skipped elements).
      - Instead you should specify the *limit* and the *startKey* with
        which it should starts looking.
    - *Changes feed*.
      - We already used that mechanism inside the configuration
        module, described in [[Configuration Patterns]].
        - Modules: *follow*.
      - How to store information that certain change was already applied?
        - You can store it inside the docs, or use the *database
          sequences* mechanism to store it.
        - Remember that sequences for multiple workers have to be
          synchronized and agreed across all workers.
