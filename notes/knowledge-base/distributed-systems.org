* Distributed Systems

** Definitions

*** Scalability

It is an ability of a system, network, or process, to handle a growing amount
of work in a capable manner or its ability to be enlarged to accommodate
that growth.

*** Performance

Is characterized by the amount of useful work accomplished by a computer
system compared to the time and resources used.

*** Latency

The state of being latent; delay, a period between the initiation of
something and the occurrence.

**** Latent

From Latin latens, latentis, present participle of lateo (lie hidden).
Existing or present but concealed or inactive.

*** Availability

The proportion of time a system is in a functioning condition. If a user
cannot access the system, it is said to be unavailable.

Adding new machine does not increase overall performance of distributed
system linearly.

There are many things which hold this back:
- Latency, synchronization, infrastructure, geographic distribution.
- As bigger cluster become then the performance increase is smaller.

*** Replication

Way of copying the same cached data and structure in multiple places in
order to reduce latency and data availability.

Increasing amount of replicated data increases problems with
cohesion and consistency.

*** Partition

Splitting data set into smaller parts physically (also geographically)
distributed. Routing is a problem here.

*** Fault-Tolerance

Ability of a system to behave in a well-defined manner once faults occur.

Different faults:
- Anomaly is an unexpected thing.
- Error is an expected, erroneous situation.

*** Costs of Scaling

- Operational - more services, data, machines, infrastructure to handle.
- Financial - buy more in terms of amount and more expensive machines.
- Development - changing paradims and adjusting can be expensive.

*** Actor Model

Coined for the first time In J.C.R. Licklider's paper about computation
model for artificial intelligence. It is a first paper related with
distributed systems (1968).

*** Back-pressure

It's a term from plumbing. Big pipes are delivering huge volume of water
to the smaller pipes in the people houses. You don't want to bust open
smaller pipes because of higher volume in the main pipe.

Instead you're applying certain techniques which handles enhanced volume
and redirect pressure or buffer the water in your system.

The same analogy applies to the distributed systems.

*** System model

A set of assumptions about the environment and facilities on which
a distributed system is implemented.

**** Synchronous system model

Processes execute in lock-step; there is a known upper bound on message
transmission delay, each process has an accurate clock.

**** Asynchronous system model

No timing assumptions - e.g. processes execute at independent rates,
there is no bound on message transmission delay, useful clocks do
not exist.

*** Consensus

More formally:
- Agreement - Every correct process must agree on the same value.
- Integrity - Every correct process decides at most one value,
  and if it decides some value, then it must have been proposed
  by some process.
- Termination - All processes eventually reach a decision.
- Validity - If all correct processes propose the same value V,
  then all correct processes decide V.

*** FLP impossibility result

The FLP impossibility result (named after the authors Fischer, Lynch
and Patterson) examines the consensus problem under the asynchronous
system model.

Technically, the agreement problem, which is a very weak form of
the consensus problem.

It is assumed that nodes can only fail by crashing, that the network
is reliable, and that the typical timing assumptions of the asynchronous
system model hold: e.g. there are no bounds on message delay.

Under these assumptions, the FLP result states that:

There does not exist a (deterministic) algorithm for the consensus problem
in an asynchronous system subject to failures, even if messages can never
be lost, at most one process may fail, and it can only fail by crashing
(stopping executing).

This result means that there is no way to solve the consensus problem
under a very minimal system model in a way that cannot be delayed
forever.

*** CAP Theorem

- Consistency - all nodes see the same data at the same time.
- Availability - node failures do not prevent survivors from
  continuing to operate.
- Partition tolerance - the system continues to operate despite
  message loss due to network and/or node failure. This applies not
  only to the network, but general system availability.

Algorithms:
- 2PC - Two phase commit.
- Paxos
- Gossip

CAP Interpretation:
- C In CAP is a strong consistency model, but not every consistency
  model is a strong one.
  - CA (Consistency + Availability).
    - Examples include full strict quorum protocols, such as
      two-phase commit.
  - CP (Consistency + Partition tolerance).
    - Examples include majority quorum protocols in which minority
      partitions are unavailable such as Paxos.
  - AP (Availability + Partition tolerance).
    - Examples include protocols using conflict resolution,
      such as Dynamo.

In practice you have to choose P, especially in networking environment
(network is unreliable, there is always a chance that it will break) so
there is only choice between CP and AP.

Regarding partition tolerance - CP system provides serializability
(linearizable), but it should have majority it forces re-election.
However invalid algorithm used also can ruin CP features. AP systems
simply don't care about partitions, it is a normal business even
if the network partition happen.

For example Cassandra is a Dynamo system, but it does not use Vector
Clocks, but *LWW* (Last Write Wins). Why? Because of performance.
You don't want to do a read before you write data.

*** Consistency Models

A contract between programmer and system, wherein the system guarantees
that if the programmer follows some specific rules, the results of
operations on the data store will be predictable.

Strong consistency models (capable of maintaining a single copy):
- Linearizable consistency (atomic operations, in order, global clock).
- Sequential consistency.

Weak consistency models (not strong):
- Client-centric consistency models.
- Causal consistency - strongest model available.
- Eventual consistency models.

*** Consistency

- Distributed Transaction
  - MSDTC (Microsoft Distributed Transaction Coordinator)
  - Two-phase Commit (2PC)

*** Time and Order

- Partial Order is more accurate for distributed systems.
- Global Order is specific to the sequential, non-distributed execution.
- Time in distributed systems:
  - Global Clock - very hard to achieve, even by using
    dedicated protocols like NTP.
    - Even than there are systems which relies on this (e.g. Cassandra).
  - Local Clock - very popular to use this behavior in distributed systems.
    Rely only on local clock for ordering events, do not let
    the clock to jump across machines.
  - No Clock - if you can ignore time, the best and safest choice is to
    ignore it completely. Vector clocks and Lamport's clocks are
    generalization of this approach.
- Several distributed systems (e.g. Riak or Voldemort) are using one
  of these generalizations.

**** Lamport Clock

Lamport clock is simple idea. Each process maintains a counter using the
following rules:
- Whenever a process does work, increment the counter.
- Whenever a process sends a message, include the counter.
- When a message is received, set the counter to:
  - `max(local_counter, received_counter) + 1`

**** Vector Clock

Vector clock is an extension of Lamport clock, which maintains an array
`[ t1, t2, ... ]` of N logical clocks - one per each node.

Rather than incrementing a common counter, each node increments its own
logical clock in the vector by one on each internal event.

Hence the update rules are:
- Whenever a process does work, increment the logical
  clock value of the node in the vector
- Whenever a process sends a message, include the
  full vector of logical clocks

When a message is received:
- Update each element in the vector to be `max(local, received)`.
- Increment the logical clock value representing the current
  node in the vector.

*** Failure Detectors

The amount of time spent (`time for cutoff`) waiting can provide
clues about whether a system is partitioned or merely experiencing
high latency.

In this case, we don't need to assume a global clock of perfect
accuracy - it is simply enough that there is a reliable-enough
local clock.

Failure detectors are implemented using heartbeat messages and
timers. Processes exchange heartbeat messages.

If a message response is not received before the timeout occurs,
then the process suspects the other process.

Properties of failure detectors:
- Strong completeness.
  - Every crashed process is eventually suspected by every correct
    process.
- Weak completeness.
  - Every crashed process is eventually suspected by some correct
    process.
- Strong accuracy.
  - No correct process is suspected ever.
- Weak accuracy.
  - Some correct process is never suspected.

*** CALM Theorem (Consistency As Logical Monotonicity)

The idea is that the family of eventually consistent programs are
exactly those that can be expressed in monotonic logic.

By contrast, distributed non-monotonicity (e.g. destructive state
modification, aggregation or reduce) can and must be resolved via
distributed coordination logic (e.g. 2PC, Paxos).

The idea that temporary inconsistencies work out amounts to
ensuring that the data in question is contained within a
properly-protected monotonic component of the system.

*** Postel's Law

Be conservative in what you do, be liberal in what you accept from others.

*** Replication

You can replicate state synchronously or asynchronously.
Methods:
- Replication methods with strong consistency
- Replication methods that prevent divergence (single copy systems).

Implementations:
- 2PC - Two-phase commit is not partition tolerant, it won't
  survive multiple node and master failures.
- Partition tolerant consensus algorithms:
  - Paxos - full-blown implementation and algorithm for majority
    votes decisions, which survives multiple failures.
    Used in Google's Chubby Lock Manager.
  - Raft - created to be easier taught than Paxos.
    Implemented in etcd.
  - ZAB - Zookeeper Atomic Broadcast, heart of Zookeeper and
    similar tools (Storm, Kafka, Hadoop).

**** Replication methods with weak consistency

- Replication methods that risk divergence (multi-master systems).
- Eventual consistency with probabilistic guarantees.
  - Amazon Dynamo.
- Base for others:
  - Cassandra
  - Voldemort
  - Riak
    - Replication is based on partial quorums.
      - `R + W > N`
        - `W` -  nodes which have to write with success.
        - `R` - nodes which have consistent read.
        - `N` - amount of nodes in the system.
      - Examples:
        - `R = 1`, `W = N`
          - Fast reads, slow writes.
        - `R = N`, `W = 1`
          - Fast writes, slow reads.
        - `R = N/2` and `W = N/2 + 1`
          - Favorable to both.

***** Conflicts?

- Vector clocks.
  - No metadata
- Timestamps.
- Version numbers.

**** Replica Synchronization

- Gossip is a probabilistic technique for synchronizing replicas.
- Merkle trees.
- Eventual consistency with strong guarantees.
- CRDT (Convergent Replicated Data Types).
  - It exploits knowledge regarding the commutativity and associativity
    of specific operations on specific data types.
  - Data structures:
    - Counters
    - Registers
    - Sets
    - Graphs
    - Text Sequences

** Tips and Tricks

- Feature flags for operational and infrastructural changes.
  - The easiest way to deploy and test new feature is to provide
    a feature toggle with throttling and percentages of
    users/traffic/load redirected to new system.
  - It's a kind of A/B testing for infrastructure and
    operational changes.
  - It is a if statement in your code.
  - Why not a policy/strategy/any other design pattern?
    - It's a fair trade - trading local complexity to global resiliency.
    - If it fails - turn it off.
    - If it goes well - after a few days you can clean up your feature
      toggle panel and remove the if statements from the code.
- Exploit data locality
  - Process and transform the data as close as the data lives as you can.
  - Have cache where the data lives.
- Other tricks:
  - Batching.
  - Collapsed forwarding (used by CDNs).
  - Prefer services - not fat, full-fledged clients.

** Patterns

*** Circuit Breaker

- The basic idea behind the circuit breaker is very simple. You wrap
  a protected function call in a circuit breaker object, which monitors
  for failures.
- Once the failures reach a certain threshold, the circuit breaker trips,
  and all further calls to the circuit breaker return with an error,
  without the protected call being made at all.
- Usually you'll also want some kind of monitor alert if the circuit
  breaker trips.
- This simple circuit breaker avoids making the protected call when the
  circuit is open, but would need an external intervention to reset it
  when things are well again.
- This is a reasonable approach with electrical circuit breakers in buildings,
  but for software circuit breakers we can have the breaker itself detect
  if the underlying calls are working again.
- We can implement this self-resetting behavior by trying the protected
  call again after a suitable interval, and resetting the breaker should
  it succeed.
- Circuit breakers are a valuable place for monitoring. Any change in
  breaker state should be logged and breakers should reveal details of
  their state for deeper monitoring.
- Breaker behavior is often a good source of warnings about deeper
  troubles in the environment. Operations staff should be able to
  trip or reset breakers.
